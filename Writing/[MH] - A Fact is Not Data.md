
[[2024-05-11]]
# Recommendation

1. **Consider the Opposite**: When you come to a quick conclusion, try to challenge it by considering the opposite viewpoint. Ask yourself, "What if I'm wrong?" or "What evidence would prove the opposite of my assumption?" This can help counteract confirmation bias, where you only look for information that supports your beliefs.
    
2. **Think Like a Scientist**: Treat your initial belief or opinion as a hypothesis rather than a fact. Look for data and evidence that tests this hypothesis. This approach can help you remain objective and open to changing your mind based on new information, avoiding the anchoring bias where you rely too heavily on the first piece of information you receive.
    
3. **Play Devil’s Advocate**: Regularly question your conclusions by arguing against them. This can be especially useful in group settings where groupthink might lead everyone to agree without sufficient scrutiny.
    
4. **Use the 10/10/10 Rule**: To help with decision-making biases, think about how you will feel about your decision in 10 minutes, 10 months, and 10 years. This can help you look beyond immediate reactions and consider longer-term consequences, which might be overlooked.
    
5. **Checklists and Predefined Criteria**: Before making a decision, create a checklist or set criteria based on what’s important and stick to it. This can help reduce the impact of emotions or irrelevant information on your decision-making process.
    
6. **Seek Diverse Opinions**: Actively seek out and listen to people who have different perspectives from your own. This can broaden your view and decrease the likelihood of falling prey to biases like the echo chamber effect, where you only hear opinions that echo your own.
    
7. **Be Aware of Overconfidence**: Remind yourself of times when you were sure but wrong, to temper the tendency to be overly confident in your judgments. Recognizing the limits of your knowledge can make you more cautious and thorough in evaluating information.


Devil, Explorative, Ten, Experiment, Checklist, Humble (DETECH)

The Devil's Advocate
The Tomb Raider
The Ten Triplet
The Scientist
The Doctor
The Humble yourself

----


“You’ve just read two of the most famous explanations for Apple’s success. The first is from the multimillion-selling biography of Steve Jobs by Walter Isaacson. The second is from Simon Sinek’s ‘How great leaders inspire action’, the thirdmost-watched TED talk of all time, with over 60 million views and the basis for his book Start with Why. Yet these two accounts are completely different from one another.

But it’s their similarities that help us understand how two inconsistent explanations became lodged in Apple lore.* Both prey on black-and-white thinking. Adoption may have negative as well as positive effects, and launching a company based on a why rather than a business plan might seem like building a house on sand. However, if we’re predisposed to think something is unequivocally good or bad, we accept the explanation and ignore moderation or marbling.
Both accounts also play into confirmation bias. We like underdogs, so we root for adopted kids. We want to believe that a why leads to success because that’s empowering. Perhaps Apple’s fortunes instead stemmed from a eureka moment or a stellar network of contacts – yet many companies have neither, so any book claiming they’re the key is unlikely to become a hit. But anyone can come up with a why if they think hard enough, hold long enough brainstorming sessions or hire expensive enough consultants. Believing Sinek puts you in the driving seat.”

A third similarity is that both authors tell a compelling narrative. They make it seem like Apple’s success was logical – predestined, even – given the company’s why or its CEO’s childhood. To do this, they reverse-engineer a story for Apple’s success and make it vivid, exciting and appealing. If they only need to explain a single example, authors are free to concoct whatever account they want. They can then highlight some choice factoids to support it and ignore anything that doesn’t fit. Neither considers the possibility of alternative explanations, which is why two highly popular accounts don’t even acknowledge each other.

“We’ve seen how the twin biases can lead us astray, but why is reverse engineering a problem? Is it really so bad to come up with an explanation after the event? Jobs himself didn’t think so. He explained in a famous 2005 graduation speech at Stanford University that ‘You can’t connect the dots looking forward; you can only connect them looking backwards.’ Shouldn’t we use the benefit of hindsight, when all the facts are on the table – to see all the dots before we try to join them? And why do we complain about brushing aside what doesn’t match the narrative? Isn’t the skill of an author, a journalist or even an academic researcher to see clarity in chaos: to tune out the noise and focus on the signal?

To understand the flaws in this popular approach, let’s dip into one of the most influential streams of finance research at the turn of the millennium.”

----


Everyone has ‘that friend’ who makes similar boasts; let’s call ours Dietrich. He’ll boast about how he made a killing on crypto, a fortune on foreign exchange or a surplus on stocks. You take those claims with a pinch of salt. The problem isn’t the facts that you do see – if Dietrich says he made a 76% profit last year, you take his word for it – but the facts that you don’t. You suspect that three other friends, Aanya, Bruno and Chloe, are dabbling in day trading themselves. Yet they’ve never mentioned how they’ve done, probably because they came a cropper. Since only Dietrich is vaunting his investment returns, you have a selected sample. This shows why a fact is not data : it may not be representative. Even if Dietrich’s return claims are true, they’re meaningless, as they don’t tell you how successful day traders are in general.

Before taking the plunge and going into amateur investing yourself, you’d first like to see the profits and losses of all your friends. But you can’t force people to disclose their investment record;2 even if a professor pleaded for volunteers in the name of scientific research, probably only those who struck it rich would come forward. You’d need to be pretty intrepid to get the data you need.

“This dataset allowed Terry to write many seminal papers on investor behaviour, mostly with Brad Barber at the University of California at Davis. One influential study calculated the profit that frequent traders make.† Importantly, they studied all frequent traders in their sample, regardless of whether they struck it rich; they simply picked out every single hyperactive investor without pre-screening for their level of success, and calculated the average return across the group. Recall that Reyes the Entrepreneur claimed to have earned 3.311% in a single day. If he enjoyed that return in each of the 253 trading days in a year, that aggregates to 379,286%.‡ When Brad and Terry looked at a representative sample of antsy shareholders, it wasn’t even close. The average day trader earned a measly 11.4% each year.

----



> [!quote]
> The difference between 379,286% and 11.4% is overwhelming. Yet the conclusion might seem underwhelming. No day trader expects to earn 379,286%. Their main hope is to get something positive, to win more than they lose, and 11.4% per year is still comfortably above zero. If you invested for ten years at this average, no better, no worse, your total return would be 194% – you’d triple your money. So Terry spent all that effort, steeled himself to make hundreds of brazen requests, and dealt with rejection after rejection, to no avail. The end result was the same: frequent trading makes money. You won’t win as much as 379,286%, but you’ll still win big.”
> 
> But here’s the twist. Seeing the full picture involves calculating two numbers, and the average profit of restless investors is only the first. The second question to ask is: How much money would you make if you weren’t a frequent trader? What if you simply invested in the overall stock market, rather than trying to pick a few winners – and then left your portfolio alone instead of chopping and changing it each time news broke out? The alternative – what would have happened otherwise – is known as the counterfactual. 
> 
> Brad and Terry found that a buy-and-hold investor, who bought the market and didn’t make a single trade, would have earned 17.9% per year. The 11.4% return to frequent trading should be compared not with zero but with 17.9%. This implies a very different, and sobering, conclusion from Reyes and Dietrich. All that time and effort amateur investors put into trading actually worsens their long-term financial security.
> 
> The implications are profound. We might think freedom and individual choice are good, as citizens know what’s best for them and shouldn’t be dictated to by governments. But, left to their own devices, they may take actions that hurt themselves. If so, policymakers might design nudges that encourage the public to invest in broad-based funds rather than individual stocks. Sure, you won’t make 379,286% a year – you won’t get rich quick – but you will get rich.


---


# Hypothesis-testing thinking

1. **Form a Hypothesis**: This is like a smart guess. Brad and Terry guessed that "frequent trading affects returns," which means they think that if someone trades a lot, it might change how much money they make from investing.
    
2. **Gather a Sample**: Since it's impossible to look at every single investor who trades a lot, Brad and Terry needed to pick a group of traders to study. They had to make sure this group (or sample) was a good mix of different types of traders and not just the best or the worst ones. It’s like if you only tasted the frosting of a cake, you wouldn’t really know what the whole cake tastes like. You need a piece that has a bit of everything: frosting, sponge, and filling.
    
3. **Find a Control Sample**: This is a group that doesn’t do the thing you're studying—in this case, trading a lot. For Brad and Terry, this was investors who buy and hold their investments and don’t trade much. This helps to see if the results from the traders are really because of trading a lot, or just because the market in general is doing well.
    
4. **Calculate Averages**: They look at how much money both groups (frequent traders and buy-and-hold investors) made on average. Let's say the frequent traders made 11.4% and the buy-and-hold ones made 17.9%. This step helps them see the difference between the two groups.
    
5. **Statistical Significance**: This is a fancy way to figure out if the difference they see (like the 11.4% vs 17.9%) is likely because of trading a lot or just by chance. They checked how big the gap is (6.5% difference) and how many people were in the study (over 13,000 investors for six years). They found there’s only a 0.1% chance that this big of a difference would happen just by luck, which is really, really small. Usually, if the chance is below 5%, they think it’s significant—meaning it's likely not just due to luck.


> [!quote] 
> Let’s now highlight each step that Brad and Terry followed, to form a general playbook that we can apply to any question we explore with data, such as what caused Apple’s success. The first step is to state your question in the form of a hypothesis about how an input affects an output. For Brad and Terry, it’s that ‘frequent trading affects returns'.
> 
> You then test the hypothesis. For this, you’d ideally like the trading records of every single trigger-happy investor. That’s impossible, so the second step is to gather a sample. What’s critical is that the sample is representative, not selected – it captures a broad mix of traders rather than pre-screening them on some criterion, such as whether they volunteered to share their record or had an account for five years (both of which would skew the sample to more successful investors). That’s similar to how you’d sample a cake by cutting it vertically so that your slice contains the icing, sponge, filling and base, rather than splitting it horizontally and skimming off only the icing. The extensive compilation of excitable shareholders is known as the test sample – you’re testing whether it performs better.
> 
> Step three is equally critical – to find a control sample that doesn’t have the input. The high returns to fidgety investors might be nothing to do with the input (frequent trading) but just because the market went up. So you need to find out how much was earned by buy-and-hold investors who didn’t trade at all. Step four is to calculate the average output across the two samples, which gives you the 11.4% and 17.9%.
> 
> You’re tempted to conclude that frequent trading lowers returns, but there’s one final step. Even if frequent trading has no effect on profits, it could still underperform due to luck. In fact, there’s a 50% probability that it does, just like a fair coin with no bias towards heads might still land heads in a single toss. So you can’t just look at whether jumpy shareholders beat or lag the market; you need to take into account two other factors. The first is the magnitude of the underperformance. What matters isn’t just that 11.4% is lower than 17.9% but that it’s 6.5% lower. The second is the sample size – how many frequent traders Brad and Terry studied, and for how long. A single punter like Reyes might get lucky over one day, but the researchers analysed 13,293 jittery investors over six years.
> 
> We need to work out how likely it is that you get a gap as large as 6.5% from 13,293 investors over six years by happenstance – even if trading frequency didn’t affect returns. If the probability is small, then the result is unlikely to have been a product of chance and so it supports your hypothesis. Let’s say Brad and Terry crunched the numbers and found that the likelihood was 0.1%.# How improbable must a difference be before you interpret it as supporting your hypothesis? It’s subjective, but the convention is to have a threshold of 5%, which is known as a 5% significance level. Since 0.1% is well below 5%, Brad and Terry’s result would be statistically significant, allowing them to draw the conclusion that frequent trading affects returns.
> 
> It seems obvious that the magnitude and sample size matter, and so the need to calculate statistical significance should be clear. If I told you that a coin landed heads more than half the time, you wouldn’t immediately label it loaded; you’d want to know how much it beat the midpoint by and over how many tosses. 3 heads out of 5 is barely above halfway (2.5), but 40 heads out of 50 would be striking.
> 
> Yet you only need to glance at a LinkedIn feed, Instagram story, or newspaper’s daily digest to be flooded with headlines such as ‘People who do X are more successful,’ ‘Companies with Y are more profitable’ and ‘Countries with Z are happier.’ None of these headlines mention how much they outperformed by or for how long, or how many people, companies or countries were studied – yet we lap them up. Even if X, Y and Z were completely irrelevant, there’s half a chance they’d still be associated with success.
> 
> Despite the power of statistical significance, note that it can never prove a hypothesis, so we should be sceptical of claims such as ‘indisputable evidence’ or ‘proof ’. Statistical significance simply means that it’s unlikely that the result is due to chance – but it’s not impossible. On 18 August 1913, at the famous Casino de Monte-Carlo, the roulette wheel landed on black twenty-six times straight, even though the odds are 1 in 66.6 million.** Even this highly improbable sequence doesn’t prove that the wheel was biased – it could have been fair but just went on a very lucky streak. Or a very unlucky one, depending on where you placed your bets. 
> 
> The whole playbook – forming a hypothesis, gathering representative test and control samples, testing for statistical significance and only then reaching a conclusion – is the scientific method. But Isaacson used a quite different approach. Rather than starting with a hypothesis, he jumped to a conclusion. He identified a number of factors that he claimed were behind Jobs’s success, such as his adoption by the right parents, upbringing and focus. Chapter 1 is entitled ‘Childhood: abandoned and chosen’ and its first section is ‘The adoption’, so we’ll discuss the first factor, but the same concerns apply to the others.

---

### Why a Fact Isn't Enough

Using just one fact to make a broad conclusion is risky because it doesn't give us the whole picture. It's like saying because one swan you saw is white, all swans must be white. This overlooks any swans that might not be white. Here’s why this matters:

1. **Context and Completeness**: Data provides context. For example, knowing that Steve Jobs was adopted doesn’t tell us much about why he was successful. But if we had data on lots of CEOs who were adopted (and those who weren't), we could start to see if there’s a real pattern or just a coincidence.
    
2. **Generalization**: A single fact about one person or event can be an outlier—the exception rather than the rule. Data helps us see what's typical and what's not by showing trends and averages across many cases.
    
3. **Bias in Selection**: If you only pick facts that support your belief (like only looking at successful CEOs who were adopted), you might ignore evidence that doesn’t fit your theory. This is called selection bias. Data helps reduce this bias by including a wider range of examples.

> [!quote]
> While Isaacson considers only Jobs, others have pointed out how Amazon’s Jeff Bezos4 and Oracle’s Larry Ellison were also adopted and deduce that adoption must drive success. Such a conclusion is based on facts. Jobs, Bezos and Ellison were all definitely adopted, and all ended up undeniably successful. However, these facts are meaningless, because facts are not data.
> 
> You can’t just take a selected sample of adopted CEOs who made it big, just like you can’t focus only on frequent traders that brag about their winnings. You need a representative sample of dozens – ideally, hundreds – of adopted CEOs, both those who struck gold and those who didn’t, and then to calculate their average level of success. And even if most adopted CEOs hit the jackpot, that could be due to an economic upswing rather than adoption. So you also take a control sample of non-adopted CEOs and compute their average success to get the counterfactual. Then you compare the two groups and check for statistical significance.
> 
> The scientific method requires you to sample all the data. You take a database of CEOs, without pre-screening them for either their success or their adoption status. This database needs to include adopted CEOs who failed and non-adopted CEOs who succeeded, rather than just hand-picking successful adoptees because they fit the picture.
> 
> There’s nothing wrong with telling entertaining stories if you frame them as just that – stories. You can even conjecture about what led to Jobs’s success, as long as you’re clear it’s speculation. As a biography focusing only on Jobs, Isaacson’s book was fascinating and deserved its popularity. 
> 
> The problem arises when you turn a single anecdote into a general rule. A Harvard Business Review article by Isaacson was titled ‘The real leadership lessons of Steve Jobs’, with the strapline ‘Six months after Jobs’s death, the author of his best-selling biography identifies the practices that every CEO can try to emulate’; 5 HBR’s editorial promised readers ‘Yes – you, too, can be like Steve Jobs.’ You can’t change whether you’re adopted, so Isaacson instead glorified Jobs’s management principles, such as his focus and simplicity, declaring them as a blueprint for all bosses to follow. But Isaacson had no evidence that these principles worked. He didn’t study any other CEO besides Jobs who followed them, nor the hundreds of executives who’d reached the top through a different path. 
> 
> The same problem crops up in our previous examples. We’ve seen that Belle Gibson’s story was false, and stressed the importance of checking the facts. Yet just checking the facts isn’t enough. Even if Belle did beat cancer through diet, this would still provide no evidence that diet is an effective treatment. For that, you’d need to study all cancer patients who tried diet as a cure and see how many of them won their fight‡‡ – but you’ll never hear the cases where diet failed, so you have a selected sample.
> 
> The biggest problem with Belle’s story isn’t that it’s false. It’s that it’s only one story. There might be thousands of other stories where clean eating didn’t work, but those stories are too ordinary, so they never see the light of day. It’s the outlier cases that are new, and so only they make the news. 
> 
> In Peak, Anders Ericsson acknowledges that he had a selected sample. He analysed students who’d already got into Berlin’s elite Academy of Music and found that many had practised for 10,000 hours. He didn’t demonstrate that practising for 10,000 hours gets you into the academy. ‘To show a result like this, I would have needed to put a collection of randomly chosen people through ten thousand hours of deliberate practice on the violin and then see how they turned out.’ There may be hundreds of other hopefuls who’d put in 10,000 hours but didn’t make it through the doors.
> 
> Gladwell writes in Outliers that ‘The striking thing about Ericsson’s study is that he and his colleagues couldn’t find any . . . “grinds”, people who worked harder than everyone else, yet just didn’t have what it takes to break the top ranks.’ That’s because those ‘grinds’ didn’t get into the academy – so they’d have never appeared in Ericsson’s sample to begin with. In the Fortune interview he explained how ‘The premise of this book is that you can learn a lot more about success by looking around at the successful person.’ But this premise is wrong; you need to study failures too.


----

